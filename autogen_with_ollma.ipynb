{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running AutoGen with Bing Search API \n",
    "\n",
    "1) With OpenAI \n",
    "\n",
    "2) With Local Model installed through ollama\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Download https://ollama.com/\n",
    "\n",
    "for list of models (Initial EMPTY) >> ollama list \n",
    "\n",
    ">> ollama pull llama3.2 (To run >>  ollama run llama3.2)\n",
    "\n",
    ">> ollama pull gemma2:2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 : Check if openai is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "llm_config = {\n",
    "    \"config_list\": [{\"model\": \"gpt-4o-mini\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}],\n",
    "}\n",
    "assistant = AssistantAgent(\"assistant\", llm_config=llm_config)\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config=False, human_input_mode=\"NEVER\",max_consecutive_auto_reply=1,)\n",
    "\n",
    "# Start the chat\n",
    "user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"what is the meaning of life. Answer in one line\",\n",
    ")\n",
    "\n",
    "print(assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 : Check if ollama is Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl  http://localhost:11434/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent, UserProxyAgent\n",
    "\n",
    "local_llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"llama3.2\",  # \n",
    "            \"api_key\": \"ollama\",  # \n",
    "            \"base_url\": \"http://localhost:11434/v1\",  # Your URL\n",
    "            \"price\": [0, 0],  # Put in price per 1K tokens [prompt, response] as free!\n",
    "        }\n",
    "    ],\n",
    "    \"cache_seed\": None,  # Turns off caching, useful for testing different models\n",
    "}\n",
    "\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "\n",
    "# Create the agent that uses the LLM.\n",
    "assistant = ConversableAgent(\"agent\", llm_config=local_llm_config)\n",
    "# Create the agent that represents the user in the conversation.\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config=False, human_input_mode=\"NEVER\",max_consecutive_auto_reply=1,)\n",
    "\n",
    "# Let the assistant start the conversation.  It will end when the user types exit.\n",
    "res = assistant.initiate_chat(user_proxy, message=\"what is the meaning of life. Answer in one line\")\n",
    "\n",
    "print(assistant)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
